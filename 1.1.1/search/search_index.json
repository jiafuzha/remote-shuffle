{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Remote Shuffle Plugins Introduction Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote datastore, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture. There are two shuffle plugins in this project. - shuffle-hadoop, A remote shuffle plugin based Hadoop filesystem. This plugin can work with any remote filesystems compatible with Hadoop, like HDFS, AWS S3 and DAOS . - shuffle-daos Different from the above general plugin based on Hadoop Filesystem interface, this plugin bases on DAOS Object API. Thanks to DAOS Distribution Key and Attribute Key, we can improve performance by constructing shuffle output like below. Developer Guide Build and Deploy We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build using the following command in remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package User Guide (shuffle-hadoop) Enable Remote Shuffle Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir Configurations Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values. Shuffle Root Directory This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle Index Cache Size This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m Number of Threads Reading Data Files This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5 Number of Threads Transitioning Index Files (when index cache is enabled) This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3 Bypass-merge-sort Threshold This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1 Configurations fetching port for HDFS When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070 Inherited Spark Shuffle Configurations These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions Deprecated Spark Shuffle Configurations These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts Performance Evaluation Tool Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle User Guide (shuffle-daos) Most of User Guide (shuffle-hadoop) can be applied to shuffle-daos. We'll not repeat them here. Just show differences here. Shuffle Manager spark.shuffle.manager org.apache.spark.shuffle.daos.DaosShuffleManager Classpath spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar","title":"Remote Shuffle Plugins"},{"location":"#remote-shuffle-plugins","text":"","title":"Remote Shuffle Plugins"},{"location":"#introduction","text":"Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote datastore, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture. There are two shuffle plugins in this project. - shuffle-hadoop, A remote shuffle plugin based Hadoop filesystem. This plugin can work with any remote filesystems compatible with Hadoop, like HDFS, AWS S3 and DAOS . - shuffle-daos Different from the above general plugin based on Hadoop Filesystem interface, this plugin bases on DAOS Object API. Thanks to DAOS Distribution Key and Attribute Key, we can improve performance by constructing shuffle output like below.","title":"Introduction"},{"location":"#developer-guide","text":"","title":"Developer Guide"},{"location":"#build-and-deploy","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build using the following command in remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package","title":"Build and Deploy"},{"location":"#user-guide-shuffle-hadoop","text":"","title":"User Guide (shuffle-hadoop)"},{"location":"#enable-remote-shuffle","text":"Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir","title":"Enable Remote Shuffle"},{"location":"#configurations","text":"Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values.","title":"Configurations"},{"location":"#shuffle-root-directory","text":"This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle","title":"Shuffle Root Directory"},{"location":"#index-cache-size","text":"This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m","title":"Index Cache Size"},{"location":"#number-of-threads-reading-data-files","text":"This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5","title":"Number of Threads Reading Data Files"},{"location":"#number-of-threads-transitioning-index-files-when-index-cache-is-enabled","text":"This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3","title":"Number of Threads Transitioning Index Files (when index cache is enabled)"},{"location":"#bypass-merge-sort-threshold","text":"This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1","title":"Bypass-merge-sort Threshold"},{"location":"#configurations-fetching-port-for-hdfs","text":"When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070","title":"Configurations fetching port for HDFS"},{"location":"#inherited-spark-shuffle-configurations","text":"These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions","title":"Inherited Spark Shuffle Configurations"},{"location":"#deprecated-spark-shuffle-configurations","text":"These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts","title":"Deprecated Spark Shuffle Configurations"},{"location":"#performance-evaluation-tool","text":"Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle","title":"Performance Evaluation Tool"},{"location":"#user-guide-shuffle-daos","text":"Most of User Guide (shuffle-hadoop) can be applied to shuffle-daos. We'll not repeat them here. Just show differences here.","title":"User Guide (shuffle-daos)"},{"location":"#shuffle-manager","text":"spark.shuffle.manager org.apache.spark.shuffle.daos.DaosShuffleManager","title":"Shuffle Manager"},{"location":"#classpath","text":"spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar","title":"Classpath"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.1-spark-3.1.1 corresponds to all OAP modules' tag version v1.1.1-spark-3.1.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.1-spark-3.1.1 corresponds to all OAP modules' tag version v1.1.1-spark-3.1.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"User-Guide/","text":"Remote Shuffle Plugins Introduction Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote datastore, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture. There are two shuffle plugins in this project. - shuffle-hadoop, A remote shuffle plugin based Hadoop filesystem. This plugin can work with any remote filesystems compatible with Hadoop, like HDFS, AWS S3 and DAOS . - shuffle-daos Different from the above general plugin based on Hadoop Filesystem interface, this plugin bases on DAOS Object API. Thanks to DAOS Distribution Key and Attribute Key, we can improve performance by constructing shuffle output like below. Developer Guide Build and Deploy We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build using the following command in remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package User Guide (shuffle-hadoop) Enable Remote Shuffle Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir Configurations Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values. Shuffle Root Directory This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle Index Cache Size This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m Number of Threads Reading Data Files This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5 Number of Threads Transitioning Index Files (when index cache is enabled) This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3 Bypass-merge-sort Threshold This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1 Configurations fetching port for HDFS When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070 Inherited Spark Shuffle Configurations These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions Deprecated Spark Shuffle Configurations These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts Performance Evaluation Tool Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle User Guide (shuffle-daos) Most of User Guide (shuffle-hadoop) can be applied to shuffle-daos. We'll not repeat them here. Just show differences here. Shuffle Manager spark.shuffle.manager org.apache.spark.shuffle.daos.DaosShuffleManager Classpath spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar","title":"User Guide"},{"location":"User-Guide/#remote-shuffle-plugins","text":"","title":"Remote Shuffle Plugins"},{"location":"User-Guide/#introduction","text":"Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote datastore, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture. There are two shuffle plugins in this project. - shuffle-hadoop, A remote shuffle plugin based Hadoop filesystem. This plugin can work with any remote filesystems compatible with Hadoop, like HDFS, AWS S3 and DAOS . - shuffle-daos Different from the above general plugin based on Hadoop Filesystem interface, this plugin bases on DAOS Object API. Thanks to DAOS Distribution Key and Attribute Key, we can improve performance by constructing shuffle output like below.","title":"Introduction"},{"location":"User-Guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"User-Guide/#build-and-deploy","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build using the following command in remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package","title":"Build and Deploy"},{"location":"User-Guide/#user-guide-shuffle-hadoop","text":"","title":"User Guide (shuffle-hadoop)"},{"location":"User-Guide/#enable-remote-shuffle","text":"Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir","title":"Enable Remote Shuffle"},{"location":"User-Guide/#configurations","text":"Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values.","title":"Configurations"},{"location":"User-Guide/#shuffle-root-directory","text":"This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle","title":"Shuffle Root Directory"},{"location":"User-Guide/#index-cache-size","text":"This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m","title":"Index Cache Size"},{"location":"User-Guide/#number-of-threads-reading-data-files","text":"This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5","title":"Number of Threads Reading Data Files"},{"location":"User-Guide/#number-of-threads-transitioning-index-files-when-index-cache-is-enabled","text":"This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3","title":"Number of Threads Transitioning Index Files (when index cache is enabled)"},{"location":"User-Guide/#bypass-merge-sort-threshold","text":"This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1","title":"Bypass-merge-sort Threshold"},{"location":"User-Guide/#configurations-fetching-port-for-hdfs","text":"When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070","title":"Configurations fetching port for HDFS"},{"location":"User-Guide/#inherited-spark-shuffle-configurations","text":"These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions","title":"Inherited Spark Shuffle Configurations"},{"location":"User-Guide/#deprecated-spark-shuffle-configurations","text":"These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts","title":"Deprecated Spark Shuffle Configurations"},{"location":"User-Guide/#performance-evaluation-tool","text":"Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle","title":"Performance Evaluation Tool"},{"location":"User-Guide/#user-guide-shuffle-daos","text":"Most of User Guide (shuffle-hadoop) can be applied to shuffle-daos. We'll not repeat them here. Just show differences here.","title":"User Guide (shuffle-daos)"},{"location":"User-Guide/#shuffle-manager","text":"spark.shuffle.manager org.apache.spark.shuffle.daos.DaosShuffleManager","title":"Shuffle Manager"},{"location":"User-Guide/#classpath","text":"spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/daos-java-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/hadoop-daos-<version>.jar $HOME/miniconda2/envs/oapenv/oap_jars/shuffle-daos-<version>.jar","title":"Classpath"}]}